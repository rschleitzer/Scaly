package scaly 0.1.0

use scaly.memory.Page
use scaly.containers.Array
use scaly.containers.String

; Literal types
define StringLiteral(value: String)
define CharacterLiteral(value: String)
define FragmentLiteral(value: String)
define IntegerLiteral(value: String)
define BooleanLiteral(value: bool)
define FloatingPointLiteral(value: String)
define HexLiteral(value: String)

define Literal union (
    String: StringLiteral,
    Character: CharacterLiteral,
    Fragment: FragmentLiteral,
    Integer: IntegerLiteral,
    Boolean: BooleanLiteral,
    FloatingPoint: FloatingPointLiteral,
    Hex: HexLiteral
)

; Token types
define EmptyToken()
define InvalidToken()
define ColonToken()
define IdentifierToken(name: String)
define AttributeToken(name: String)
define PunctuationToken(sign: u8)
define LiteralToken(value: Literal)

define Token union (
    Empty: EmptyToken,
    Invalid: InvalidToken,
    Colon: ColonToken,
    Identifier: IdentifierToken,
    Attribute: AttributeToken,
    Punctuation: PunctuationToken,
    Literal: LiteralToken
)

; Lexer state
define Lexer
(
    source: String
    current: size_t
    token: Token
    position: size_t
    previous_position: size_t
)
{
    init (source: String)
    {
        set this.source: source
        set current: 0
        set token: Token.Empty(EmptyToken())
        set position: 0
        set previous_position: 0
    }

    function is_at_end(this: Lexer) returns bool
        current >= source.length()

    procedure empty(this: Lexer)
        set token: Token.Empty(EmptyToken())

    procedure advance#(rp, this: Lexer)
    {
        set previous_position: position
        skip_whitespace(true)

        if is_at_end()
        {
            set token: Token.Empty(EmptyToken())
            return
        }

        let c source.get(current)

        ; Identifier or keyword
        if is_identifier_start(c)
        {
            set token: scan_identifier#()
            return
        }

        ; Attribute (@name)
        if c = "@" as u8
        {
            set token: scan_attribute#()
            return
        }

        ; String literal
        if c = "\"" as u8
        {
            set token: scan_string_literal#()
            return
        }

        ; Freeform identifier (single quotes)
        if c = "'" as u8
        {
            set token: scan_freeform_identifier#()
            return
        }

        ; Numeric literal
        if is_digit(c)
        {
            set token: scan_numeric_literal#()
            return
        }

        ; Colon (special - used as separator)
        if c = ":" as u8
        {
            set current: current + 1
            set position: current
            set token: Token.Colon(ColonToken())
            return
        }

        ; Single-line comment
        if c = ";" as u8
        {
            if current + 1 < source.length() & source.get(current + 1) = "*" as u8
            {
                handle_multi_line_comment()
                advance#()
                return
            }
            handle_single_line_comment()
            advance#()
            return
        }

        ; Punctuation and operators
        if is_punctuation(c)
        {
            set current: current + 1
            set position: current
            set token: Token.Punctuation(PunctuationToken(c))
            return
        }

        ; Operator characters
        if is_operator_char(c)
        {
            set token: scan_operator#()
            return
        }

        ; Fragment literal
        if c = "`" as u8
        {
            set token: scan_fragment_literal#()
            return
        }

        ; Invalid character
        set current: current + 1
        set token: Token.Invalid(InvalidToken())
    }

    procedure skip_whitespace(this: Lexer, skip_linefeed: bool)
    {
        while current < source.length()
        {
            let c source.get(current)
            if (c = " " as u8) | (c = "\t" as u8) | (c = "\r" as u8)
            {
                set current: current + 1
                continue
            }
            if skip_linefeed & (c = "\n" as u8)
            {
                set current: current + 1
                continue
            }
            break
        }
        set position: current
    }

    procedure handle_single_line_comment(this: Lexer)
    {
        while current < source.length() & source.get(current) <> "\n" as u8
            set current: current + 1
    }

    procedure handle_multi_line_comment(this: Lexer)
    {
        set current: current + 2  ; Skip ;*
        var nesting_level 1
        while current + 1 < source.length() & nesting_level > 0
        {
            if source.get(current) = ";" as u8 & source.get(current + 1) = "*" as u8
            {
                set nesting_level: nesting_level + 1
                set current: current + 2
                continue
            }
            if source.get(current) = "*" as u8 & source.get(current + 1) = ";" as u8
            {
                set nesting_level: nesting_level - 1
                set current: current + 2
                continue
            }
            set current: current + 1
        }
    }

    function scan_identifier#(rp, this: Lexer) returns Token
    {
        let start current
        while current < source.length() & is_identifier_char(source.get(current))
            set current: current + 1

        let name source.substring(rp, start, current - start)
        set position: current

        ; Check for boolean literals
        if name.equals("true")
            return Token.Literal(LiteralToken(Literal.Boolean(BooleanLiteral(true))))
        if name.equals("false")
            return Token.Literal(LiteralToken(Literal.Boolean(BooleanLiteral(false))))

        Token.Identifier(IdentifierToken(name))
    }

    function scan_attribute#(rp, this: Lexer) returns Token
    {
        set current: current + 1  ; Skip @
        let start current
        while current < source.length() & is_identifier_char(source.get(current))
            set current: current + 1

        let name source.substring(rp, start, current - start)
        set position: current
        Token.Attribute(AttributeToken(name))
    }

    function scan_operator#(rp, this: Lexer) returns Token
    {
        let start current
        while current < source.length() & is_operator_char(source.get(current))
            set current: current + 1

        let name source.substring(rp, start, current - start)
        set position: current
        Token.Identifier(IdentifierToken(name))
    }

    function scan_string_literal#(rp, this: Lexer) returns Token
    {
        set current: current + 1  ; Skip opening "
        let start current

        while current < source.length() & source.get(current) <> "\"" as u8
        {
            if source.get(current) = "\\" as u8
                set current: current + 1  ; Skip escape
            set current: current + 1
        }

        let value source.substring(rp, start, current - start)

        if current < source.length()
            set current: current + 1  ; Skip closing "

        set position: current
        Token.Literal(LiteralToken(Literal.String(StringLiteral(value))))
    }

    function scan_freeform_identifier#(rp, this: Lexer) returns Token
    {
        set current: current + 1  ; Skip opening '
        let start current

        while current < source.length() & source.get(current) <> "'" as u8
        {
            if source.get(current) = "\\" as u8
                set current: current + 1  ; Skip escape
            set current: current + 1
        }

        let value source.substring(rp, start, current - start)

        if current < source.length()
            set current: current + 1  ; Skip closing '

        set position: current
        ; Return as identifier, not character literal
        Token.Identifier(IdentifierToken(value))
    }

    function scan_fragment_literal#(rp, this: Lexer) returns Token
    {
        set current: current + 1  ; Skip opening `
        let start current

        while current < source.length() & source.get(current) <> "`" as u8
            set current: current + 1

        let value source.substring(rp, start, current - start)

        if current < source.length()
            set current: current + 1  ; Skip closing `

        set position: current
        Token.Literal(LiteralToken(Literal.Fragment(FragmentLiteral(value))))
    }

    function scan_numeric_literal#(rp, this: Lexer) returns Token
    {
        let start current

        ; Check for hex literal (0x...)
        if source.get(current) = "0" as u8 & current + 1 < source.length()
        {
            let next source.get(current + 1)
            if next = "x" as u8 | next = "X" as u8
            {
                set current: current + 2
                while current < source.length() & is_hex_digit(source.get(current))
                    set current: current + 1
                let value source.substring(rp, start + 2, current - start - 2)
                set position: current
                return Token.Literal(LiteralToken(Literal.Hex(HexLiteral(value))))
            }
        }

        ; Integer part
        while current < source.length() & is_digit(source.get(current))
            set current: current + 1

        ; Check for decimal point
        if current < source.length() & source.get(current) = "." as u8
        {
            set current: current + 1
            while current < source.length() & is_digit(source.get(current))
                set current: current + 1

            ; Check for exponent
            if current < source.length()
            {
                let exp_char source.get(current)
                let e_lower: u8 = "e" as u8
                if exp_char = e_lower | exp_char = "E" as u8
                {
                    set current: current + 1
                    ; Optional sign
                    if current < source.length()
                    {
                        let sign_char source.get(current)
                        if sign_char = "+" as u8 | sign_char = "-" as u8
                            set current: current + 1
                    }
                    while current < source.length() & is_digit(source.get(current))
                        set current: current + 1
                }
            }

            let value source.substring(rp, start, current - start)
            set position: current
            return Token.Literal(LiteralToken(Literal.FloatingPoint(FloatingPointLiteral(value))))
        }

        let value source.substring(rp, start, current - start)
        set position: current
        Token.Literal(LiteralToken(Literal.Integer(IntegerLiteral(value))))
    }

    ; Parser helper methods
    function parse_keyword#(rp, this: Lexer, keyword: String) returns bool
    {
        choose token
            when Identifier: id
            {
                if id.name.equals(keyword)
                {
                    advance#()
                    return true
                }
            }
        false
    }

    function parse_identifier#(rp, this: Lexer) returns String
    {
        choose token
            when Identifier: id
            {
                let name id.name
                advance#()
                return name
            }
        String()
    }

    function parse_punctuation#(rp, this: Lexer, c: u8) returns bool
    {
        choose token
            when Punctuation: p
            {
                if p.sign = c
                {
                    advance#()
                    return true
                }
            }
        false
    }

    function parse_colon#(rp, this: Lexer) returns bool
    {
        choose token
            when Colon: c
            {
                advance#()
                return true
            }
        false
    }

    function parse_attribute#(rp, this: Lexer) returns String
    {
        choose token
            when Attribute: a
            {
                let name a.name
                advance#()
                return name
            }
        String()
    }
}

; Character classification helpers
function is_identifier_start(c: u8) returns bool
    (c >= "a" as u8 & c <= "z" as u8) | (c >= "A" as u8 & c <= "Z" as u8) | c = "_" as u8

function is_identifier_char(c: u8) returns bool
    is_identifier_start(c) | is_digit(c)

function is_digit(c: u8) returns bool
    c >= "0" as u8 & c <= "9" as u8

function is_hex_digit(c: u8) returns bool
    is_digit(c) | (c >= "a" as u8 & c <= "f" as u8) | (c >= "A" as u8 & c <= "F" as u8)

function is_operator_char(c: u8) returns bool
    c = "+" as u8 | c = "-" as u8 | c = "*" as u8 | c = "/" as u8 | c = "=" as u8 | c = "<" as u8 | c = ">" as u8 | c = "&" as u8 | c = "|" as u8 | c = "~" as u8 | c = "^" as u8 | c = "%" as u8

function is_punctuation(c: u8) returns bool
    c = "(" as u8 | c = ")" as u8 | c = "[" as u8 | c = "]" as u8 | c = "{" as u8 | c = "}" as u8 | c = "," as u8 | c = "." as u8

; Test helper classification functions
function test_helpers() returns int
{
    ; Test is_identifier_start
    if ~is_identifier_start("a" as u8)
        return -1
    if ~is_identifier_start("Z" as u8)
        return -2
    if ~is_identifier_start("_" as u8)
        return -3
    if is_identifier_start("0" as u8)
        return -4
    if is_identifier_start(" " as u8)
        return -5

    ; Test is_digit
    if ~is_digit("0" as u8)
        return -6
    if ~is_digit("9" as u8)
        return -7
    if is_digit("a" as u8)
        return -8

    ; Test is_hex_digit
    if ~is_hex_digit("0" as u8)
        return -9
    if ~is_hex_digit("a" as u8)
        return -10
    if ~is_hex_digit("F" as u8)
        return -11
    if is_hex_digit("g" as u8)
        return -12

    ; Test is_punctuation
    if ~is_punctuation("(" as u8)
        return -13
    if ~is_punctuation(")" as u8)
        return -14
    if is_punctuation(":" as u8)
        return -15

    ; Test is_operator_char
    if ~is_operator_char("+" as u8)
        return -16
    if ~is_operator_char("-" as u8)
        return -17
    if is_operator_char("a" as u8)
        return -18

    0
}

; Test string creation - use same pattern as parser.test()
function test_string_basics() returns int
{
    ; Same pattern as parser.test() - use # directly
    var s String#("Hello world!")
    let len s.get_length()
    if len <> 12
        return -1

    ; Check first and last characters
    if s.get(0) <> "H" as u8
        return -2
    if s.get(11) <> "!" as u8
        return -3

    0
}

; Test lexer creation
function test_lexer_creation() returns int
{
    var lexer Lexer(String#("hello"))

    ; Check initial state
    if lexer.current <> 0
        return -1

    ; Check initial token is Empty
    choose lexer.token
        when Empty: e
            return 0
        else
            return -2
}

; Tokenization tests

function test_tokenize_integers() returns int
{
    ; Test tokenizing an integer
    var lexer Lexer(String#("42"))
    lexer.advance#()

    ; Check that we got a Literal token
    choose lexer.token
        when Literal: lit
            ; Check that it's an Integer literal
            choose lit.value
                when Integer: int_lit
                    ; Verify the value
                    if int_lit.value.equals(String#("42"))
                        return 0
                    return -3
                else
                    return -2
        else
            return -1
}

function test_tokenize_booleans() returns int
{
    ; Test true literal
    var lexer Lexer(String#("true"))
    lexer.advance#()
    choose lexer.token
        when Literal: lit
            choose lit.value
                when Boolean: bool_lit
                    if ~bool_lit.value
                        return -1
                else
                    return -2
        else
            return -3

    ; Test false literal
    set lexer: Lexer(String#("false"))
    lexer.advance#()
    choose lexer.token
        when Literal: lit
            choose lit.value
                when Boolean: bool_lit
                    if bool_lit.value
                        return -4
                    return 0
                else
                    return -5
        else
            return -6
}

function test_tokenize_strings() returns int
{
    var lexer Lexer(String#("\"hello\""))
    lexer.advance#()
    choose lexer.token
        when Literal: lit
            choose lit.value
                when String: str_lit
                    if str_lit.value.equals(String#("hello"))
                        return 0
                    return -1
                else
                    return -2
        else
            return -3
}

function test_tokenize_identifiers() returns int
{
    var lexer Lexer(String#("foo"))
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            if id.name.equals(String#("foo"))
                return 0
            return -1
        else
            return -2
}

function test_tokenize_operators() returns int
{
    var lexer Lexer(String#("++"))
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            ; Operators are scanned as identifiers
            if id.name.equals(String#("++"))
                return 0
            return -1
        else
            return -2
}

function test_tokenize_punctuation() returns int
{
    var lexer Lexer(String#("("))
    lexer.advance#()
    choose lexer.token
        when Punctuation: p
            if p.sign = "(" as u8
                return 0
            return -1
        else
            return -2
}

function test_tokenize_sequence() returns int
{
    ; Test a sequence of tokens: "let x 42"
    var lexer Lexer(String#("let x 42"))

    ; First token: 'let' identifier
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            if ~id.name.equals(String#("let"))
                return -1
        else
            return -2

    ; Second token: 'x' identifier
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            if ~id.name.equals(String#("x"))
                return -3
        else
            return -4

    ; Third token: '42' integer literal
    lexer.advance#()
    choose lexer.token
        when Literal: lit
            choose lit.value
                when Integer: int_lit
                    if int_lit.value.equals(String#("42"))
                        return 0
                    return -5
                else
                    return -6
        else
            return -7
}

function test_tokenize_comments() returns int
{
    ; Test that single-line comments are skipped
    var lexer Lexer(String#("; comment\n42"))
    lexer.advance#()
    choose lexer.token
        when Literal: lit
            choose lit.value
                when Integer: int_lit
                    if int_lit.value.equals(String#("42"))
                        return 0
                    return -1
                else
                    return -2
        else
            return -3
}

function test_tokenize_colon() returns int
{
    var lexer Lexer(String#("foo: bar"))

    ; First token: 'foo' identifier
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            if ~id.name.equals(String#("foo"))
                return -1
        else
            return -2

    ; Second token: colon
    lexer.advance#()
    choose lexer.token
        when Colon: c
            ; Colon matched
            0
        else
            return -3

    ; Third token: 'bar' identifier
    lexer.advance#()
    choose lexer.token
        when Identifier: id
            if id.name.equals(String#("bar"))
                return 0
            return -4
        else
            return -5
}

function test_tokenize_attributes() returns int
{
    var lexer Lexer(String#("@summary"))
    lexer.advance#()
    choose lexer.token
        when Attribute: attr
            if attr.name.equals(String#("summary"))
                return 0
            return -1
        else
            return -2
}

; Main test function
function test() returns int
{
    var result test_helpers()
    if result <> 0
        return result

    set result: test_string_basics()
    if result <> 0
        return result - 10

    set result: test_lexer_creation()
    if result <> 0
        return result - 20

    ; Tokenization tests
    set result: test_tokenize_integers()
    if result <> 0
        return result - 30

    set result: test_tokenize_booleans()
    if result <> 0
        return result - 40

    set result: test_tokenize_strings()
    if result <> 0
        return result - 50

    set result: test_tokenize_identifiers()
    if result <> 0
        return result - 60

    set result: test_tokenize_operators()
    if result <> 0
        return result - 70

    set result: test_tokenize_punctuation()
    if result <> 0
        return result - 80

    set result: test_tokenize_sequence()
    if result <> 0
        return result - 90

    set result: test_tokenize_comments()
    if result <> 0
        return result - 100

    set result: test_tokenize_colon()
    if result <> 0
        return result - 110

    set result: test_tokenize_attributes()
    if result <> 0
        return result - 120

    0
}
